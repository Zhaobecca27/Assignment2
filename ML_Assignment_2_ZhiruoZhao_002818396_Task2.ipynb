{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a057e82",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## CS6140 Machine Learning\n",
    "## Zhiruo Zhao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eadb049",
   "metadata": {},
   "source": [
    "## Task 2: Classification Task\n",
    "### Dataset: Breast Cancer Wisconsin (Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145ff4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron as SKPerceptron\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d52df",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing:\n",
    "- Load and clean the data.\n",
    "- Normalize the features if necessary.\n",
    "- Apply appropriate preprocessing suitable for classification problem.\n",
    "- Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94bdfe",
   "metadata": {},
   "source": [
    "The dataset contains 569 entries with 32 columns. The key observations are:\n",
    "\n",
    "The ID column is not useful for classification and should be dropped.\n",
    "The Diagnosis column is the target variable, with values \"M\" (Malignant) and \"B\" (Benign).\n",
    "The remaining 30 columns are numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bca92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>Radius_mean</th>\n",
       "      <th>Texture_mean</th>\n",
       "      <th>Perimeter_mean</th>\n",
       "      <th>Area_mean</th>\n",
       "      <th>Smoothness_mean</th>\n",
       "      <th>Compactness_mean</th>\n",
       "      <th>Concavity_mean</th>\n",
       "      <th>Concave_points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Radius_worst</th>\n",
       "      <th>Texture_worst</th>\n",
       "      <th>Perimeter_worst</th>\n",
       "      <th>Area_worst</th>\n",
       "      <th>Smoothness_worst</th>\n",
       "      <th>Compactness_worst</th>\n",
       "      <th>Concavity_worst</th>\n",
       "      <th>Concave_points_worst</th>\n",
       "      <th>Symmetry_worst</th>\n",
       "      <th>Fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Diagnosis  Radius_mean  Texture_mean  Perimeter_mean  Area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   Smoothness_mean  Compactness_mean  Concavity_mean  Concave_points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  Radius_worst  Texture_worst  Perimeter_worst  Area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   Smoothness_worst  Compactness_worst  Concavity_worst  Concave_points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   Symmetry_worst  Fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = '../breast+cancer+wisconsin+diagnostic/wdbc.data'\n",
    "names_path = '../breast+cancer+wisconsin+diagnostic/wdbc.names'\n",
    "\n",
    "# Based on the Breast Cancer Wisconsin (Diagnostic) dataset structure:\n",
    "# The dataset has the following structure:\n",
    "# ID, Diagnosis (M = malignant, B = benign), followed by 30 real-valued features\n",
    "column_names = [\n",
    "    'ID', 'Diagnosis', 'Radius_mean', 'Texture_mean', 'Perimeter_mean', 'Area_mean',\n",
    "    'Smoothness_mean', 'Compactness_mean', 'Concavity_mean', 'Concave_points_mean',\n",
    "    'Symmetry_mean', 'Fractal_dimension_mean', 'Radius_se', 'Texture_se',\n",
    "    'Perimeter_se', 'Area_se', 'Smoothness_se', 'Compactness_se', 'Concavity_se',\n",
    "    'Concave_points_se', 'Symmetry_se', 'Fractal_dimension_se', 'Radius_worst',\n",
    "    'Texture_worst', 'Perimeter_worst', 'Area_worst', 'Smoothness_worst',\n",
    "    'Compactness_worst', 'Concavity_worst', 'Concave_points_worst', 'Symmetry_worst',\n",
    "    'Fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, names=column_names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b8f69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (114, 30), (455,), (114,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the ID column\n",
    "df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Encode the Diagnosis column\n",
    "df[\"Diagnosis\"] = LabelEncoder().fit_transform(df[\"Diagnosis\"])  # M -> 1, B -> 0\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=[\"Diagnosis\"])\n",
    "y = df[\"Diagnosis\"]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810cf07",
   "metadata": {},
   "source": [
    "# 2. Implement Gaussian NaiveBayes (GNB) and Gaussian Discriminant Analysis (GDA):\n",
    "- Use shared co-variance as well as class specific co-variance for GDA\n",
    "- Implement the fit and predict functions.\n",
    "- Train the model on the training set.\n",
    "- Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a5e405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB Accuracy:  0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "        self.priors = {}\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.mean[c] = X_c.mean(axis=0)\n",
    "            self.var[c] = X_c.var(axis=0) + 1e-9  # 避免除零\n",
    "            self.priors[c] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(x) for x in X])\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        for c in self.classes:\n",
    "            prior = np.log(self.priors[c])\n",
    "            likelihood = -0.5 * np.sum(np.log(2 * np.pi * self.var[c]))\n",
    "            likelihood -= 0.5 * np.sum(((x - self.mean[c]) ** 2) / self.var[c])\n",
    "            posterior = prior + likelihood\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "# Train GNB Model\n",
    "gnb = GaussianNaiveBayes()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "print(f\"GNB Accuracy: \",accuracy_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daeef6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDA (Shared Covariance) Accuracy: 0.9035\n",
      "GDA (Class-Specific Covariance) Accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "class GaussianDiscriminantAnalysis:\n",
    "    def fit(self, X, y, shared_covariance=True):\n",
    "        self.classes = np.unique(y)\n",
    "        self.means = {c: np.mean(X[y == c], axis=0) for c in self.classes}\n",
    "        self.priors = {c: len(X[y == c]) / len(X) for c in self.classes}\n",
    "        \n",
    "        if shared_covariance:\n",
    "            # Shared covariance matrix for all classes\n",
    "            self.covariance = np.cov(X.T)\n",
    "        else:\n",
    "            # Separate covariance matrix for each class\n",
    "            self.covariances = {c: np.cov(X[y == c].T) for c in self.classes}\n",
    "        \n",
    "        self.shared_covariance = shared_covariance\n",
    "\n",
    "    def predict(self, X):\n",
    "        posteriors = np.zeros((X.shape[0], len(self.classes)))\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            mean = self.means[c]\n",
    "            prior = np.log(self.priors[c])\n",
    "            \n",
    "            if self.shared_covariance:\n",
    "                cov_inv = np.linalg.inv(self.covariance)\n",
    "                determinant = np.linalg.det(self.covariance)\n",
    "            else:\n",
    "                cov_inv = np.linalg.inv(self.covariances[c])\n",
    "                determinant = np.linalg.det(self.covariances[c])\n",
    "            \n",
    "            diff = X - mean\n",
    "            likelihood = -0.5 * np.log(determinant) - 0.5 * np.sum(diff @ cov_inv * diff, axis=1)\n",
    "            \n",
    "            posteriors[:, i] = prior + likelihood\n",
    "\n",
    "        return self.classes[np.argmax(posteriors, axis=1)]\n",
    "\n",
    "# Train and evaluate GDA with shared covariance\n",
    "gda_shared = GaussianDiscriminantAnalysis()\n",
    "gda_shared.fit(X_train, y_train, shared_covariance=True)\n",
    "y_pred_gda_shared = gda_shared.predict(X_test)\n",
    "gda_shared_accuracy = np.mean(y_pred_gda_shared == y_test)\n",
    "\n",
    "# Train and evaluate GDA with class-specific covariance\n",
    "gda_class_specific = GaussianDiscriminantAnalysis()\n",
    "gda_class_specific.fit(X_train, y_train, shared_covariance=False)\n",
    "y_pred_gda_class_specific = gda_class_specific.predict(X_test)\n",
    "gda_class_specific_accuracy = np.mean(y_pred_gda_class_specific == y_test)\n",
    "\n",
    "print(f\"GDA (Shared Covariance) Accuracy: {gda_shared_accuracy:.4f}\")\n",
    "print(f\"GDA (Class-Specific Covariance) Accuracy: {gda_class_specific_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c27cd",
   "metadata": {},
   "source": [
    "# 3. Implement Logistic Regression:\n",
    "- Derive the Logistic Regression equations and implement the fit function using gradient\n",
    "- descent.\n",
    "- Train the model on the training set.\n",
    "- Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f7272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.theta = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros(n)  # Initialize weights\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            linear_model = np.dot(X, self.theta)\n",
    "            predictions = self.sigmoid(linear_model)\n",
    "            gradient = np.dot(X.T, (predictions - y)) / m\n",
    "            self.theta -= self.learning_rate * gradient  # Gradient descent step\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.theta)\n",
    "        probabilities = self.sigmoid(linear_model)\n",
    "        return (probabilities >= 0.5).astype(int)  # Convert probabilities to class labels\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "log_reg = LogisticRegression(learning_rate=0.01, epochs=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {log_reg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527ff59",
   "metadata": {},
   "source": [
    "# 4. Implement Perceptron:\n",
    "- Derive the Perceptron learning rule and implement the algorithm.\n",
    "- Train the model on the training set.\n",
    "- Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981463fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy: 0.9123\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        y_transformed = np.where(y == 0, -1, 1)  # Convert labels {0,1} → {-1,1}\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(m):\n",
    "                prediction = np.sign(np.dot(X[i], self.weights) + self.bias)\n",
    "                if prediction != y_transformed[i]:  # Update if misclassified\n",
    "                    self.weights += self.learning_rate * y_transformed[i] * X[i]\n",
    "                    self.bias += self.learning_rate * y_transformed[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(np.dot(X, self.weights) + self.bias >= 0, 1, 0)\n",
    "\n",
    "# Train the Perceptron model\n",
    "perceptron = Perceptron(learning_rate=0.01, epochs=1000)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_perceptron = perceptron.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "perceptron_accuracy = accuracy_score(y_test, y_pred_perceptron)\n",
    "\n",
    "print(f\"Perceptron Accuracy: {perceptron_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf4d28",
   "metadata": {},
   "source": [
    "# 5. Comparison and Analysis:\n",
    "Compare your solutions with standard APIs in terms of model parameters and appropriate metrics (e.g., accuracy, precision, recall, F1 score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee0d3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Gaussian Naïve Bayes Results:\n",
      "  - Accuracy:  0.9211\n",
      "  - Precision: 0.9231\n",
      "  - Recall:    0.8571\n",
      "  - F1 Score:  0.8889\n",
      "----------------------------------------\n",
      "Sklearn Gaussian Naïve Bayes Results:\n",
      "  - Accuracy:  0.9211\n",
      "  - Precision: 0.9231\n",
      "  - Recall:    0.8571\n",
      "  - F1 Score:  0.8889\n",
      "----------------------------------------\n",
      "Custom GDA (Shared Covariance) Results:\n",
      "  - Accuracy:  0.9035\n",
      "  - Precision: 1.0000\n",
      "  - Recall:    0.7381\n",
      "  - F1 Score:  0.8493\n",
      "----------------------------------------\n",
      "Custom GDA (Class-Specific Covariance) Results:\n",
      "  - Accuracy:  0.9474\n",
      "  - Precision: 0.9286\n",
      "  - Recall:    0.9286\n",
      "  - F1 Score:  0.9286\n",
      "----------------------------------------\n",
      "Sklearn LDA Results:\n",
      "  - Accuracy:  0.9649\n",
      "  - Precision: 1.0000\n",
      "  - Recall:    0.9048\n",
      "  - F1 Score:  0.9500\n",
      "----------------------------------------\n",
      "Custom Logistic Regression Results:\n",
      "  - Accuracy:  0.9825\n",
      "  - Precision: 0.9762\n",
      "  - Recall:    0.9762\n",
      "  - F1 Score:  0.9762\n",
      "----------------------------------------\n",
      "Sklearn Logistic Regression Results:\n",
      "  - Accuracy:  0.9737\n",
      "  - Precision: 0.9756\n",
      "  - Recall:    0.9524\n",
      "  - F1 Score:  0.9639\n",
      "----------------------------------------\n",
      "Custom Perceptron Results:\n",
      "  - Accuracy:  0.9123\n",
      "  - Precision: 0.8810\n",
      "  - Recall:    0.8810\n",
      "  - F1 Score:  0.8810\n",
      "----------------------------------------\n",
      "Sklearn Perceptron Results:\n",
      "  - Accuracy:  0.9561\n",
      "  - Precision: 0.9744\n",
      "  - Recall:    0.9048\n",
      "  - F1 Score:  0.9383\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron as SKPerceptron\n",
    "\n",
    "# Train Scikit-Learn models\n",
    "gnb_sklearn = GaussianNB().fit(X_train, y_train)\n",
    "log_reg_sklearn = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "perceptron_sklearn = SKPerceptron(max_iter=1000, eta0=0.01).fit(X_train, y_train)\n",
    "lda_sklearn = LDA().fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_gnb_sklearn = gnb_sklearn.predict(X_test)\n",
    "y_pred_log_reg_sklearn = log_reg_sklearn.predict(X_test)\n",
    "y_pred_perceptron_sklearn = perceptron_sklearn.predict(X_test)\n",
    "y_pred_lda_sklearn = lda_sklearn.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} Results:\")\n",
    "    print(f\"  - Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  - Precision: {precision:.4f}\")\n",
    "    print(f\"  - Recall:    {recall:.4f}\")\n",
    "    print(f\"  - F1 Score:  {f1:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Evaluate all models\n",
    "evaluate_model(y_test, y_pred_gnb, \"Custom Gaussian Naïve Bayes\")\n",
    "evaluate_model(y_test, y_pred_gnb_sklearn, \"Sklearn Gaussian Naïve Bayes\")\n",
    "evaluate_model(y_test, y_pred_gda_shared, \"Custom GDA (Shared Covariance)\")\n",
    "evaluate_model(y_test, y_pred_gda_class_specific, \"Custom GDA (Class-Specific Covariance)\")\n",
    "evaluate_model(y_test, y_pred_lda_sklearn, \"Sklearn LDA\")\n",
    "evaluate_model(y_test, y_pred_log_reg, \"Custom Logistic Regression\")\n",
    "evaluate_model(y_test, y_pred_log_reg_sklearn, \"Sklearn Logistic Regression\")\n",
    "evaluate_model(y_test, y_pred_perceptron, \"Custom Perceptron\")\n",
    "evaluate_model(y_test, y_pred_perceptron_sklearn, \"Sklearn Perceptron\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1f7f0",
   "metadata": {},
   "source": [
    "## **comparison table** \n",
    "\n",
    "| **Model**                           | **Accuracy** | **Precision** | **Recall** | **F1 Score** | **Notes** |\n",
    "|-------------------------------------|------------|------------|--------|------------|-----------------------------|\n",
    "| **Custom Gaussian Naïve Bayes**     | 0.9211     | 0.9231     | 0.8571 | 0.8889     | Assumes feature independence |\n",
    "| **Sklearn Gaussian Naïve Bayes**    | 0.9211     | 0.9231     | 0.8571 | 0.8889     | Matches custom implementation |\n",
    "| **Custom GDA (Shared Covariance)**  | 0.9035     | 1.0000     | 0.7381 | 0.8493     | Assumes equal covariance across classes |\n",
    "| **Custom GDA (Class-Specific)**     | 0.9474     | 0.9286     | 0.9286 | 0.9286     | More flexible than shared covariance |\n",
    "| **Sklearn LDA**                     | 0.9649     | 1.0000     | 0.9048 | 0.9500     | Optimized version of GDA |\n",
    "| **Custom Logistic Regression**      | 0.9825     | 0.9762     | 0.9762 | 0.9762     | Strong performance, well-tuned gradient descent |\n",
    "| **Sklearn Logistic Regression**     | 0.9737     | 0.9756     | 0.9524 | 0.9639     | Slightly lower than custom but still strong |\n",
    "| **Custom Perceptron**               | 0.9123     | 0.8810     | 0.8810 | 0.8810     | Struggles with non-linearly separable data |\n",
    "| **Sklearn Perceptron**              | 0.9561     | 0.9744     | 0.9048 | 0.9383     | Better optimization & convergence |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Observations\n",
    "**Gaussian Naïve Bayes (GNB)** performs well but assumes feature independence.  \n",
    "**Custom and Sklearn GDA results vary**: class-specific covariance improves accuracy significantly.  \n",
    "**Logistic Regression outperforms GDA**, proving its robustness in classification tasks.  \n",
    "**Sklearn Perceptron outperforms the custom version**, likely due to improved optimization methods.  \n",
    "**Perceptron is weaker overall**, reinforcing its limitations on linearly separable data. \n",
    "\n",
    "### Strengths and Weaknesses of Each Algorithm\n",
    "\n",
    "1. **Custom Gaussian Naïve Bayes**  \n",
    "   - **Strengths:** Simple, interpretable.  \n",
    "   - **Weaknesses:** Assumes feature independence, which may not hold in many cases.\n",
    "\n",
    "2. **Sklearn Gaussian Naïve Bayes**  \n",
    "   - **Strengths:** Fast and reliable, matches custom version.  \n",
    "   - **Weaknesses:** Same feature independence assumption.\n",
    "\n",
    "3. **Custom GDA (Shared Covariance)**  \n",
    "   - **Strengths:** Flexible, models class distributions.  \n",
    "   - **Weaknesses:** Assumes equal covariance across classes, limiting flexibility.\n",
    "\n",
    "4. **Custom GDA (Class-Specific)**  \n",
    "   - **Strengths:** More flexible, different covariance for each class.  \n",
    "   - **Weaknesses:** Risk of overfitting with limited data.\n",
    "\n",
    "5. **Sklearn LDA**  \n",
    "   - **Strengths:** Optimized, good balance for class distribution.  \n",
    "   - **Weaknesses:** Assumes equal covariance across classes.\n",
    "\n",
    "6. **Custom Logistic Regression**  \n",
    "   - **Strengths:** Excellent performance, well-tuned.  \n",
    "   - **Weaknesses:** Struggles with high-dimensional data without regularization.\n",
    "\n",
    "7. **Sklearn Logistic Regression**  \n",
    "   - **Strengths:** Optimized, good results for linear separability.  \n",
    "   - **Weaknesses:** Slightly less powerful than custom version.\n",
    "\n",
    "8. **Custom Perceptron**  \n",
    "   - **Strengths:** Simple, effective for linearly separable data.  \n",
    "   - **Weaknesses:** Struggles with non-linearly separable data.\n",
    "\n",
    "9. **Sklearn Perceptron**  \n",
    "   - **Strengths:** Faster convergence, better optimization.  \n",
    "   - **Weaknesses:** Same issues with non-linearly separable data.\n",
    "\n",
    "### Linear Separability\n",
    "\n",
    "**Linear Separability**: Data is linearly separable if a straight line (or hyperplane in higher dimensions) can separate classes.\n",
    "\n",
    "**Checking Linearly Separable Data**:\n",
    "1. **Visual Inspection:** Plot data to check for a separable line.\n",
    "2. **Model Performance:** If linear classifiers (like Logistic Regression, Perceptron) perform well, data is likely separable.\n",
    "3. **Decision Boundaries:** Plot boundaries to see if they separate classes effectively.\n",
    "\n",
    "**Conclusion**: Custom Perceptron struggled, indicating non-linear separability, while Logistic Regression and Sklearn Perceptron performed well, suggesting linear separability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002dcc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
